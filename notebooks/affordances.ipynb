{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29bef37d",
   "metadata": {},
   "source": [
    "# Affordance Heatmaps\n",
    "\n",
    "This notebook visualizes the pick and place affordance predictions of a pre-trained `multi-language-conditioned` agent from the quickstart guide.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- Set the root folder environment variable with `export CLIPORT_ROOT=<cliport_root>`\n",
    "- Complete the [quickstart guide](https://github.com/cliport/cliport#quickstart) in README.md\n",
    "- Generate `val` and `test` splits for the task you want to evaluate on by running `python cliport/demos.py n=10 mode=test task=stack-block-pyramid-seq-seen-colors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CLIPORT_ROOT=/home/chenwu/cliport\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from cliport import tasks\n",
    "from cliport import agents\n",
    "from cliport.utils import utils\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "from cliport.dataset import RavensDataset\n",
    "from cliport.environments.environment import Environment\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bd6b5a6",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_demos = 1000 # number training demonstrations used to train agent\n",
    "n_eval = 1 # number of evaluation instances\n",
    "mode = 'test' # val or test\n",
    "\n",
    "agent_name = 'cliport'\n",
    "model_task = 'multi-language-conditioned' # multi-task agent conditioned with language goals\n",
    "\n",
    "model_folder = 'cliport_quickstart' # path to pre-trained checkpoint\n",
    "ckpt_name = 'steps=400000-val_loss=0.00014655.ckpt' # name of checkpoint to load\n",
    "\n",
    "draw_grasp_lines = True\n",
    "affordance_heatmap_scale = 30\n",
    "\n",
    "### Uncomment the task you want to evaluate on ###\n",
    "# eval_task = 'align-rope'\n",
    "# eval_task = 'assembling-kits-seq-seen-colors'\n",
    "# eval_task = 'assembling-kits-seq-unseen-colors'\n",
    "# eval_task = 'packing-shapes'\n",
    "# eval_task = 'packing-boxes-pairs-seen-colors'\n",
    "# eval_task = 'packing-boxes-pairs-unseen-colors'\n",
    "# eval_task = 'packing-seen-google-objects-seq'\n",
    "# eval_task = 'packing-unseen-google-objects-seq'\n",
    "# eval_task = 'packing-seen-google-objects-group'\n",
    "# eval_task = 'packing-unseen-google-objects-group'\n",
    "# eval_task = 'put-block-in-bowl-seen-colors'\n",
    "# eval_task = 'put-block-in-bowl-unseen-colors'\n",
    "eval_task = 'stack-block-pyramid-seq-seen-colors'\n",
    "# eval_task = 'stack-block-pyramid-seq-unseen-colors'\n",
    "# eval_task = 'separating-piles-seen-colors'\n",
    "# eval_task = 'separating-piles-unseen-colors'\n",
    "# eval_task = 'towers-of-hanoi-seq-seen-colors'\n",
    "# eval_task = 'towers-of-hanoi-seq-unseen-colors'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c812da35",
   "metadata": {},
   "source": [
    "### Load Configs and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.environ['CLIPORT_ROOT']\n",
    "assets_root = os.path.join(root_dir, 'cliport/environments/assets/')\n",
    "config_file = 'eval.yaml' \n",
    "\n",
    "vcfg = utils.load_hydra_config(os.path.join(root_dir, f'cliport/cfg/{config_file}'))\n",
    "vcfg['data_dir'] = os.path.join(root_dir, 'data')\n",
    "vcfg['mode'] = mode\n",
    "\n",
    "vcfg['model_task'] = model_task\n",
    "vcfg['eval_task'] = eval_task\n",
    "vcfg['agent'] = agent_name\n",
    "\n",
    "# Model and training config paths\n",
    "model_path = os.path.join(root_dir, model_folder)\n",
    "vcfg['train_config'] = f\"{model_path}/{vcfg['model_task']}-{vcfg['agent']}-n{train_demos}-train/.hydra/config.yaml\"\n",
    "vcfg['model_path'] = f\"{model_path}/{vcfg['model_task']}-{vcfg['agent']}-n{train_demos}-train/checkpoints/\"\n",
    "\n",
    "tcfg = utils.load_hydra_config(vcfg['train_config'])\n",
    "\n",
    "# Load dataset\n",
    "ds = RavensDataset(os.path.join(vcfg['data_dir'], f'{vcfg[\"eval_task\"]}-{vcfg[\"mode\"]}'), \n",
    "                   tcfg, \n",
    "                   n_demos=n_eval,\n",
    "                   augment=False)\n",
    "\n",
    "eval_run = 0\n",
    "name = '{}-{}-{}-{}'.format(vcfg['eval_task'], vcfg['agent'], n_eval, eval_run)\n",
    "print(f'\\nEval ID: {name}\\n')\n",
    "\n",
    "# Initialize agent\n",
    "utils.set_seed(eval_run, torch=True)\n",
    "agent = agents.names[vcfg['agent']](name, tcfg, None, ds)\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = os.path.join(vcfg['model_path'], ckpt_name)\n",
    "print(f'\\nLoading checkpoint: {ckpt_path}')\n",
    "agent.load(ckpt_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a6832c0",
   "metadata": {},
   "source": [
    "### Spawn Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c138c4a",
   "metadata": {},
   "source": [
    "### Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bb611",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize environment and task.\n",
    "env = Environment(\n",
    "    assets_root,\n",
    "    disp=False,\n",
    "    shared_memory=False,\n",
    "    hz=480,\n",
    "    record_cfg=False\n",
    ")\n",
    "\n",
    "episode = 0\n",
    "num_eval_instances = min(n_eval, ds.n_episodes)\n",
    "\n",
    "for i in range(num_eval_instances):\n",
    "    print(f'\\nEvaluation Instance: {i + 1}/{num_eval_instances}')\n",
    "    \n",
    "    # Load episode\n",
    "    episode, seed = ds.load(i)\n",
    "    goal = episode[-1]\n",
    "    total_reward = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set task\n",
    "    task_name = vcfg['eval_task']\n",
    "    task = tasks.names[task_name]()\n",
    "    task.mode = mode\n",
    "    \n",
    "    # Set environment\n",
    "    env.seed(seed)\n",
    "    env.set_task(task)\n",
    "    obs = env.reset()\n",
    "    info = env.info\n",
    "    reward = 0\n",
    "    \n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    # Rollout\n",
    "    while (step <= task.max_steps) and not done:\n",
    "        print(f\"Step: {step} ({task.max_steps} max)\")\n",
    "        \n",
    "        # Get batch\n",
    "        if step == task.max_steps-1:\n",
    "            batch = ds.process_goal((obs, None, reward, info), perturb_params=None)\n",
    "        else:\n",
    "            batch = ds.process_sample((obs, None, reward, info), augment=False)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(13, 7))\n",
    "        \n",
    "        # Get color and depth inputs\n",
    "        img = batch['img']\n",
    "        # img = torch.from_numpy(img)\n",
    "        color = np.uint8(img.detach().cpu().numpy())[:,:,:3]\n",
    "        color = color.transpose(1,0,2)\n",
    "        depth = np.array(img.detach().cpu().numpy())[:,:,3]\n",
    "        depth = depth.transpose(1,0)\n",
    "        \n",
    "        # Display input color\n",
    "        axs[0,0].imshow(color)\n",
    "        axs[0,0].axes.xaxis.set_visible(False)\n",
    "        axs[0,0].axes.yaxis.set_visible(False)\n",
    "        axs[0,0].set_title('Input RGB')\n",
    "        \n",
    "        # Display input depth\n",
    "        axs[0,1].imshow(depth)\n",
    "        axs[0,1].axes.xaxis.set_visible(False)\n",
    "        axs[0,1].axes.yaxis.set_visible(False)        \n",
    "        axs[0,1].set_title('Input Depth')\n",
    "        \n",
    "        # Display predicted pick affordance\n",
    "        axs[1,0].imshow(color)\n",
    "        axs[1,0].axes.xaxis.set_visible(False)\n",
    "        axs[1,0].axes.yaxis.set_visible(False)\n",
    "        axs[1,0].set_title('Pick Affordance')\n",
    "        \n",
    "        # Display predicted place affordance\n",
    "        axs[1,1].imshow(color)\n",
    "        axs[1,1].axes.xaxis.set_visible(False)\n",
    "        axs[1,1].axes.yaxis.set_visible(False)\n",
    "        axs[1,1].set_title('Place Affordance')\n",
    "        \n",
    "        # Get action predictions\n",
    "        l = str(info['lang_goal'])\n",
    "        act = agent.act(obs, info, goal=None)\n",
    "        pick, place = act['pick'], act['place']\n",
    "        \n",
    "        # Visualize pick affordance\n",
    "        pick_inp = {'inp_img': batch['img'].unsqueeze(0).cuda(), 'lang_goal': [l]}\n",
    "        pick_conf = agent.attn_forward(pick_inp)\n",
    "        assert pick_conf.dim() == 4\n",
    "        pick_conf = pick_conf.permute(0, 2, 3, 1)\n",
    "        pick_conf = pick_conf.detach().cpu().numpy()\n",
    "        argmax = np.argmax(pick_conf.reshape(pick_conf.shape[0], -1), axis=1)\n",
    "        coord0, coord1, coord2 = np.unravel_index(argmax, shape=pick_conf.shape[1:])\n",
    "        p0_pix = np.stack([coord0, coord1], axis=1)\n",
    "        p0_theta = coord2 * (2 * np.pi / pick_conf.shape[3])\n",
    "        assert p0_pix.shape[0] == p0_theta.shape[0] == 1\n",
    "        p0_pix = p0_pix[0]\n",
    "        p0_theta = p0_theta[0]\n",
    "\n",
    "        print(\"p0_pix\", p0_pix)\n",
    "        print(\"p0_theta\", p0_theta)\n",
    "\n",
    "        pick_conf = pick_conf[0]\n",
    "        logits = pick_conf\n",
    "    \n",
    "        line_len = 30\n",
    "        pick0 = (pick[0] + line_len/2.0 * np.sin(p0_theta), pick[1] + line_len/2.0 * np.cos(p0_theta))\n",
    "        pick1 = (pick[0] - line_len/2.0 * np.sin(p0_theta), pick[1] - line_len/2.0 * np.cos(p0_theta))\n",
    "\n",
    "        if draw_grasp_lines:\n",
    "            axs[1,0].plot((pick1[0], pick0[0]), (pick1[1], pick0[1]), color='r', linewidth=1)\n",
    "        \n",
    "        # Visualize place affordance\n",
    "        place_inp = {'inp_img': batch['img'].unsqueeze(0).cuda(), \n",
    "                     'p0': torch.LongTensor(pick).unsqueeze(0).cuda(),\n",
    "                     'lang_goal': [l]}\n",
    "        place_conf = agent.trans_forward(place_inp)\n",
    "\n",
    "        assert place_conf.dim() == 4\n",
    "        place_conf = place_conf.permute(0, 2, 3, 1)\n",
    "        place_conf = place_conf.detach().cpu().numpy()\n",
    "        argmax = np.argmax(place_conf.reshape(place_conf.shape[0], -1), axis=1)\n",
    "        coord0, coord1, coord2 = np.unravel_index(argmax, shape=place_conf.shape[1:])\n",
    "        p1_pix = np.stack([coord0, coord1], axis=1)\n",
    "        p1_theta = coord2 * (2 * np.pi / place_conf.shape[3]) * -1.0\n",
    "        assert p1_pix.shape[0] == p1_theta.shape[0] == 1\n",
    "        p1_pix = p1_pix[0]\n",
    "        p1_theta = p1_theta[0]\n",
    "\n",
    "        place_conf = place_conf[0]\n",
    "        print(\"p1_pix\", p1_pix)\n",
    "        print(\"p1_theta\", p1_theta)\n",
    "        \n",
    "        line_len = 30\n",
    "        place0 = (place[0] + line_len/2.0 * np.sin(p1_theta), place[1] + line_len/2.0 * np.cos(p1_theta))\n",
    "        place1 = (place[0] - line_len/2.0 * np.sin(p1_theta), place[1] - line_len/2.0 * np.cos(p1_theta))\n",
    "\n",
    "        if draw_grasp_lines:\n",
    "            axs[1,1].plot((place1[0], place0[0]), (place1[1], place0[1]), color='g', linewidth=1)\n",
    "\n",
    "        # Overlay affordances on RGB input\n",
    "        pick_logits_disp = np.uint8(logits * 255 * affordance_heatmap_scale).transpose(1,0,2)\n",
    "        place_logits_disp = np.uint8(np.sum(place_conf, axis=2)[:,:,None] * 255 * affordance_heatmap_scale).transpose(1,0,2)    \n",
    "\n",
    "        pick_logits_disp_masked = np.ma.masked_where(pick_logits_disp < 0, pick_logits_disp)\n",
    "        place_logits_disp_masked = np.ma.masked_where(place_logits_disp < 0, place_logits_disp)\n",
    "\n",
    "        axs[1][0].imshow(pick_logits_disp_masked, alpha=0.75)\n",
    "        axs[1][1].imshow(place_logits_disp_masked, cmap='viridis', alpha=0.75)\n",
    "        \n",
    "        print(f\"Lang Goal: {str(info['lang_goal'])}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Act with the predicted actions\n",
    "        obs, reward, done, info = env.step(act)\n",
    "        step += 1\n",
    "        \n",
    "    if done:\n",
    "        print(\"Done. Success.\")\n",
    "    else:\n",
    "        print(\"Max steps reached. Task failed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
